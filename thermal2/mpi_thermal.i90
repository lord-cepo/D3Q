# 1 "/gpfsdswork/projects/rech/yhk/unr46rr/qe-7.2-tetra/D3Q/thermal2/mpi_thermal.f90"
!
! Written by Lorenzo Paulatto (2016) IMPMC @ UPMC / CNRS UMR7590
!  Dual licenced under the CeCILL licence v 2.1
!  <http://www.cecill.info/licences/Licence_CeCILL_V2.1-fr.txt>
!  and under the GPLv2 licence and following, see
!  <http://www.gnu.org/copyleft/gpl.txt>
!
! hack:
# 14


MODULE mpi_thermal
   USE kinds,  ONLY : DP
   USE functions,        ONLY : default_if_not_present
!USE timers, ONLY : t_mpicom
# 22


# 1 "/gpfsdswork/projects/rech/yhk/unr46rr/qe-7.2-tetra/D3Q/thermal2/mpi_thermal.h" 1 

# 4










!IMPLICIT NONE
!#define i2c(a) = TRIM(int_to_char(a))
!CHARACTER (LEN=6), EXTERNAL :: int_to_char

# 25 "/gpfsdswork/projects/rech/yhk/unr46rr/qe-7.2-tetra/D3Q/thermal2/mpi_thermal.f90" 2 

!dir$ message "----------------------------------------------------------------------------------------------"
# 29

!dir$ message "Compiling _without_ MPI support (mpi subroutines will do nothing)"

!
# 35

!dir$ message "Compiling _without_ OpenMP directives"

!dir$ message "----------------------------------------------------------------------------------------------"

   INTEGER :: my_id=0, num_procs=1, ierr
   LOGICAL :: ionode = .TRUE. ! everyone is ionode before I start MPI
   LOGICAL :: mpi_started = .FALSE.
   INTEGER :: omp_tot_thr=1

   INTERFACE mpi_broadcast
      MODULE PROCEDURE mpi_bcast_logical
!
      MODULE PROCEDURE mpi_bcast_scl
      MODULE PROCEDURE mpi_bcast_vec
      MODULE PROCEDURE mpi_bcast_mat
      MODULE PROCEDURE mpi_bcast_tns
      MODULE PROCEDURE mpi_bcast_tns4
!
      MODULE PROCEDURE mpi_bcast_zscl
      MODULE PROCEDURE mpi_bcast_zvec
      MODULE PROCEDURE mpi_bcast_zmat
      MODULE PROCEDURE mpi_bcast_ztns
      MODULE PROCEDURE mpi_bcast_ztns4
!
      MODULE PROCEDURE mpi_bcast_integer
      MODULE PROCEDURE mpi_bcast_integer_vec
      MODULE PROCEDURE mpi_bcast_integer_mat
!
      MODULE PROCEDURE mpi_bcast_character
   END INTERFACE
!
   INTERFACE mpi_bsum
      MODULE PROCEDURE mpi_bsum_int
      MODULE PROCEDURE mpi_bsum_ivec

      MODULE PROCEDURE mpi_bsum_scl
      MODULE PROCEDURE mpi_bsum_vec
      MODULE PROCEDURE mpi_bsum_mat
      MODULE PROCEDURE mpi_bsum_tns
      MODULE PROCEDURE mpi_bsum_tns4

      MODULE PROCEDURE mpi_bsum_zscl
      MODULE PROCEDURE mpi_bsum_zvec
      MODULE PROCEDURE mpi_bsum_zmat
      MODULE PROCEDURE mpi_bsum_ztns
      MODULE PROCEDURE mpi_bsum_ztns4
   END INTERFACE


CONTAINS

   SUBROUTINE start_mpi()
      IMPLICIT NONE
# 91

# 98

      my_id = 0
      num_procs = 1
      ionode = .true.
      IF(ionode) WRITE(*,"(2x,a,i6,a)") "Running without MPI support"

!
# 107

      omp_tot_thr = 1

      CALL mpi_bsum(omp_tot_thr)
      IF(ionode .and. omp_tot_thr>num_procs) WRITE(*,"(2x,a,i6,a)") &
         "Using",  omp_tot_thr, " total MPI+OpenMP threads"
      mpi_started = .true.
   END SUBROUTINE

   SUBROUTINE stop_mpi()
# 119

   END SUBROUTINE

   SUBROUTINE abort_mpi(errorcode)
      IMPLICIT NONE
      INTEGER :: ierr
      INTEGER, INTENT(IN):: errorcode
# 128

   END SUBROUTINE

   SUBROUTINE mpi_wbarrier()
      IMPLICIT NONE
      INTEGER :: ierr
# 136

   END SUBROUTINE

   SUBROUTINE mpi_any(lgc)
      IMPLICIT NONE
      LOGICAL,INTENT(inout) :: lgc

# 146

   END SUBROUTINE
!
   SUBROUTINE mpi_all(lgc)
      IMPLICIT NONE
      LOGICAL,INTENT(inout) :: lgc

# 156

   END SUBROUTINE

! In-place MPI sum of integer, scalar, vector and matrix
   SUBROUTINE mpi_bsum_int(scl)
      IMPLICIT NONE
      INTEGER,INTENT(inout) :: scl
# 166

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_ivec(nn, vec)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: nn
      INTEGER,INTENT(inout) :: vec(nn)
# 176

   END SUBROUTINE
!
   SUBROUTINE mpi_block_divide(n, start, end)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: n
      INTEGER,INTENT(out) :: start, end
      INTEGER :: i, step, extra
!
      step = n/num_procs
      extra = MOD(n, num_procs)
!IF(my_id==0 ) WRITE(*,*) "blocks to divide:", n, num_procs, step, extra
!
      end = 0
      DO i = 0, my_id
         start=end+1
         end = start+step
         IF(i>=extra) end=end-1
      ENDDO
!WRITE(*,*) "block divide:", my_id, start, end, end-start
   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_scl(scl)
      IMPLICIT NONE
      REAL(DP),INTENT(inout) :: scl
# 204

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_vec(nn, vec)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: nn
      REAL(DP),INTENT(inout) :: vec(nn)
# 214

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_mat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: mm, nn
      REAL(DP),INTENT(inout) :: mat(mm,nn)
# 224

   END SUBROUTINE
!
   SUBROUTINE mpi_sum_mat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: mm, nn
      REAL(DP),INTENT(inout) :: mat(mm,nn)
      INTEGER :: rank_id
      INTEGER :: info

# 244

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_tns(ll, mm, nn, tns)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn
      REAL(DP),INTENT(inout) :: tns(ll, mm,nn)
# 254

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_tns4(ll, mm, nn, oo, tns)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn, oo
      REAL(DP),INTENT(inout) :: tns(ll, mm,nn, oo)
# 264

   END SUBROUTINE

!!  ! --------- ------------- --- -- -- -- - - - complex numbers follow
   SUBROUTINE mpi_bsum_zscl(scl)
      IMPLICIT NONE
      COMPLEX(DP),INTENT(inout) :: scl

# 275

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_zvec(nn, vec)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: nn
      COMPLEX(DP),INTENT(inout) :: vec(nn)
# 285

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_zmat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: mm, nn
      COMPLEX(DP),INTENT(inout) :: mat(mm,nn)
# 295

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_ztns(ll, mm, nn, tns)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn
      COMPLEX(DP),INTENT(inout) :: tns(ll, mm,nn)
# 305

   END SUBROUTINE
!
   SUBROUTINE mpi_bsum_ztns4(ll, mm, nn, oo, tns4)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn, oo
      COMPLEX(DP),INTENT(inout) :: tns4(ll, mm,nn, oo)
# 315

   END SUBROUTINE
!
! --------- ------------- --- -- -- -- - - - complex numbers follow
!
   SUBROUTINE mpi_bcast_logical(logi)
      IMPLICIT NONE
      LOGICAL,INTENT(inout) :: logi
# 325

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_integer(inte,root)
      IMPLICIT NONE
      INTEGER,INTENT(inout) :: inte
      INTEGER,OPTIONAL,INTENT(in) :: root
# 334

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_integer_vec(nn,inte)
      IMPLICIT NONE
      INTEGER,INTENT(in)    :: nn
      INTEGER,INTENT(inout) :: inte(nn)
# 343

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_integer_mat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: mm, nn
      INTEGER,INTENT(inout) :: mat(mm,nn)
# 352

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_character(schar)
      IMPLICIT NONE
      CHARACTER(len=*),INTENT(inout) :: schar
# 360

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_scl(scl)
      IMPLICIT NONE
      REAL(DP),INTENT(inout) :: scl
# 368

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_vec(nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     ::  nn
      REAL(DP),INTENT(inout) :: mat(nn)
# 377

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_mat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: mm, nn
      REAL(DP),INTENT(inout) :: mat(mm,nn)
# 386

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_tns(ll, mm, nn, tns)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn
      REAL(DP),INTENT(inout) :: tns(ll, mm,nn)
# 395

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_tns4(ll, mm, nn, oo, tns4)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn, oo
      REAL(DP),INTENT(inout) :: tns4(ll, mm,nn, oo)
# 404

   END SUBROUTINE
!
! Now broadcast for complex numbers
!
   SUBROUTINE mpi_bcast_zscl(scl)
      IMPLICIT NONE
      COMPLEX(DP),INTENT(inout) :: scl
# 414

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_zvec(nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     ::  nn
      COMPLEX(DP),INTENT(inout) :: mat(nn)
# 423

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_zmat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: mm, nn
      COMPLEX(DP),INTENT(inout) :: mat(mm,nn)
# 432

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_ztns(ll, mm, nn, tns, root)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn
      COMPLEX(DP),INTENT(inout) :: tns(ll, mm,nn)
      INTEGER,OPTIONAL,INTENT(in) :: root
# 442

   END SUBROUTINE
!
   SUBROUTINE mpi_bcast_ztns4(ll, mm, nn, oo, tns4)
      IMPLICIT NONE
      INTEGER,INTENT(in)     :: ll, mm, nn, oo
      COMPLEX(DP),INTENT(inout) :: tns4(ll, mm,nn, oo)
# 451

   END SUBROUTINE
!
!!
! Scatter in-place a vector
   SUBROUTINE scatteri_vec(nn, vec, ii)
      IMPLICIT NONE
      INTEGER,INTENT(inout)  :: nn
      REAL(DP),INTENT(inout),ALLOCATABLE :: vec(:)
      INTEGER,OPTIONAL,INTENT(out)  :: ii
!
      INTEGER  :: nn_send, nn_recv
      REAL(DP),ALLOCATABLE :: vec_send(:), vec_recv(:)
!
      IF(.not.allocated(vec)) CALL errore('scatteri_vec', 'input vector must be allocated', 1)
      IF(size(vec)/=nn)      CALL errore('scatteri_vec', 'input vector must be of size nn', 2)
!
# 478

! do nothing

   END SUBROUTINE
!
!  ! Scatter in-place a vector
!  SUBROUTINE gatheri_vec(nn, vec, ii)
!    IMPLICIT NONE
!    INTEGER,INTENT(inout)  :: nn
!    REAL(DP),INTENT(inout),ALLOCATABLE :: vec(:)
!    INTEGER,OPTIONAL,INTENT(out)  :: ii
!    !
!    INTEGER  :: nn_send, nn_recv
!    REAL(DP),ALLOCATABLE :: vec_send(:), vec_recv(:)
!    !
!    IF(.not.allocated(vec)) CALL errore('scatteri_vec', 'input vector must be allocated', 1)
!    IF(size(vec)/=nn)      CALL errore('scatteri_vec', 'input vector must be of size nn', 2)
!    !
!#ifdef __MPI
!    nn_send = nn
!    ALLOCATE(vec_send(nn_send))
!    vec_send(1:nn_send) = vec(1:nn_send)
!    CALL gather_vec(nn_send, vec_send, nn_recv, vec_recv, ii)
!    DEALLOCATE(vec)
!    ALLOCATE(vec(nn_recv))
!    vec(1:nn_recv) = vec_recv(1:nn_recv)
!    nn = nn_recv
!#else
!    ! do nothing
!#endif
!  END SUBROUTINE


! Scatter in-place a matrix, along the second dimension
   SUBROUTINE scatteri_mat(mm, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: mm
      INTEGER,INTENT(inout)  :: nn
      REAL(DP),INTENT(inout),ALLOCATABLE :: mat(:,:)
!
      INTEGER  :: nn_send, nn_recv
      REAL(DP),ALLOCATABLE :: mat_send(:,:), mat_recv(:,:)
!
      IF(.not.allocated(mat)) CALL errore('scatteri_mat', 'input matrix must be allocated', 1)
      IF(size(mat,1)/=mm)      CALL errore('scatteri_mat', 'input matrix must be of size mm*nn', 2)
      IF(size(mat,2)/=nn)      CALL errore('scatteri_mat', 'input matrix must be of size mm*nn', 3)
!
# 535

! do nothing

   END SUBROUTINE

   SUBROUTINE scatteri_tns(mm, ll, nn, mat)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: mm, ll
      INTEGER,INTENT(in)  :: nn
      REAL(DP),INTENT(inout),ALLOCATABLE :: mat(:,:,:)
!
      INTEGER  :: nn_send, nn_recv
      REAL(DP),ALLOCATABLE :: mat_send(:,:,:), mat_recv(:,:,:)
!
      IF(.not.allocated(mat)) CALL errore('scatteri_mat', 'input matrix must be allocated', 1)
      IF(size(mat,1)/=mm)      CALL errore('scatteri_mat', 'input matrix must be of size mm*ll*nn', 2)
      IF(size(mat,2)/=ll)      CALL errore('scatteri_mat', 'input matrix must be of size mm*ll*nn', 3)
      IF(size(mat,3)/=nn)      CALL errore('scatteri_mat', 'input matrix must be of size mm*ll*nn', 4)

!
# 564

! do nothing

   END SUBROUTINE

! Divide a vector among all the CPUs
   SUBROUTINE scatter_vec(nn_send, vec_send, nn_recv, vec_recv, ii_recv)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: nn_send
      REAL(DP),INTENT(in) :: vec_send(nn_send)
      INTEGER,INTENT(out)  :: nn_recv
      REAL(DP),ALLOCATABLE,INTENT(out) :: vec_recv(:)
      INTEGER,OPTIONAL,INTENT(out)  :: ii_recv
!
      INTEGER :: nn_residual, i
      INTEGER,ALLOCATABLE  :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='scatter_vec'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)
!    IF(num_procs>nn_send) CALL errore(sub, 'num_procs > nn_send, this can work but makes no sense', 1)

# 607

      nn_recv = nn_send
      IF(allocated(vec_recv)) DEALLOCATE(vec_recv)
      ALLOCATE(vec_recv(nn_recv))
      vec_recv = vec_send
      IF(present(ii_recv)) ii_recv = 0

   END SUBROUTINE

! Divide a vector among all the CPUs
   SUBROUTINE allgather_vec(nn_send, vec_send, vec_recv) !, ii_recv)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: nn_send
      REAL(DP),INTENT(in) :: vec_send(nn_send)
      REAL(DP),INTENT(out) :: vec_recv(:)
!INTEGER,OPTIONAL,INTENT(out)  :: ii_recv
!
!INTEGER :: nn_summed, i
      INTEGER :: i, nn_recv
      INTEGER,ALLOCATABLE  :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='scatter_vec'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)
!    IF(num_procs>nn_send) CALL errore(sub, 'num_procs > nn_send, this can work but makes no sense', 1)

# 657

!nn_recv = nn_send
!IF(allocated(vec_recv)) DEALLOCATE(vec_recv)
!ALLOCATE(vec_recv(nn_recv))
      vec_recv = vec_send

   END SUBROUTINE allgather_vec


! Divide a vector among all the CPUs
   SUBROUTINE gather_vec(nn_send, vec_send, vec_recv) !, ii_recv)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: nn_send
      REAL(DP),INTENT(in) :: vec_send(nn_send)
      REAL(DP),ALLOCATABLE,INTENT(out) :: vec_recv(:)
!INTEGER,OPTIONAL,INTENT(out)  :: ii_recv
!
!INTEGER :: nn_summed, i
      INTEGER :: i, nn_recv
      INTEGER,ALLOCATABLE  :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='scatter_vec'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)
!    IF(num_procs>nn_send) CALL errore(sub, 'num_procs > nn_send, this can work but makes no sense', 1)

# 708

      nn_recv = nn_send
      IF(allocated(vec_recv)) DEALLOCATE(vec_recv)
      ALLOCATE(vec_recv(nn_recv))
      vec_recv = vec_send

   END SUBROUTINE
!
! Divide a vector among all the CPUs
   SUBROUTINE gather_mat(mm, nn_send, vec_send, vec_recv) !, ii_recv)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: mm, nn_send
      REAL(DP),INTENT(in) :: vec_send(mm, nn_send)
      REAL(DP),ALLOCATABLE,INTENT(out) :: vec_recv(:,:)
!INTEGER,OPTIONAL,INTENT(out)  :: ii_recv
!
!INTEGER :: nn_summed, i
      INTEGER :: i, nn_recv, mmnn_send
      INTEGER,ALLOCATABLE  :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='scatter_vec'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)
!    IF(num_procs>nn_send) CALL errore(sub, 'num_procs > nn_send, this can work but makes no sense', 1)

# 762

      nn_recv = nn_send
      IF(allocated(vec_recv)) DEALLOCATE(vec_recv)
      ALLOCATE(vec_recv(mm,nn_recv))
      vec_recv = vec_send

   END SUBROUTINE

! Divide a matrix, along the last dimension, among all the CPUs
   SUBROUTINE scatter_mat(mm, nn_send, mat_send, nn_recv, mat_recv)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: mm, nn_send
      REAL(DP),INTENT(in) :: mat_send(mm,nn_send)
      INTEGER,INTENT(out)  :: nn_recv
      REAL(DP),ALLOCATABLE,INTENT(out) :: mat_recv(:,:)
!
      INTEGER :: nn_residual, i
      INTEGER,ALLOCATABLE  :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='scatter_mat'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)
!    IF(num_procs>nn_send) CALL errore(sub, 'num_procs > nn_send, this can work but makes no sense', 1)

# 808

      nn_recv = nn_send
      IF(allocated(mat_recv)) DEALLOCATE(mat_recv)
      ALLOCATE(mat_recv(mm,nn_recv))
      mat_recv = mat_send


   END SUBROUTINE

   SUBROUTINE scatter_tns(mm, ll, nn_send, mat_send, nn_recv, mat_recv)
      IMPLICIT NONE
      INTEGER,INTENT(in)  :: mm, ll, nn_send
      REAL(DP),INTENT(in) :: mat_send(mm,ll,nn_send)
      INTEGER,INTENT(out)  :: nn_recv
      REAL(DP),ALLOCATABLE,INTENT(out) :: mat_recv(:,:,:)
!
      INTEGER :: nn_residual, i
      INTEGER,ALLOCATABLE  :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='scatter_mat'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)
!    IF(num_procs>nn_send) CALL errore(sub, 'num_procs > nn_send, this can work but makes no sense', 1)

# 854

      nn_recv = nn_send
      IF(allocated(mat_recv)) DEALLOCATE(mat_recv)
      ALLOCATE(mat_recv(mm,ll,nn_recv))
      mat_recv = mat_send


   END SUBROUTINE

   SUBROUTINE allgather_mat(mm, nn_send, vec_send, vec_recv)
      IMPLICIT NONE
      INTEGER, INTENT(in)  :: mm, nn_send
      REAL(DP), INTENT(in) :: vec_send(mm, nn_send)
      REAL(DP), ALLOCATABLE, INTENT(out) :: vec_recv(:,:)
!
      INTEGER :: nn_recv, mmnn_send
      INTEGER,ALLOCATABLE :: nn_scatt(:), ii_scatt(:)
      CHARACTER(len=11),PARAMETER :: sub='allgather_mat'
!
      IF(.not.mpi_started) CALL errore(sub, 'MPI not started', 1)

# 898

      nn_recv = nn_send
      IF(allocated(vec_recv)) DEALLOCATE(vec_recv)
      ALLOCATE(vec_recv(mm, nn_recv))
      vec_recv = vec_send

   END SUBROUTINE allgather_mat


END MODULE mpi_thermal



